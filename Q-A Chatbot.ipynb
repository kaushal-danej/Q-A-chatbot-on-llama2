{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef296f5e-2685-47be-bf0f-935952c99c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8658f0-7981-482f-b449-23d8b1219fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whylogs==1.3.0 (from whylogs[viz]==1.3.0)\n",
      "  Using cached whylogs-1.3.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.5.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs==1.3.0->whylogs[viz]==1.3.0) (3.10.0)\n",
      "Collecting protobuf>=3.19.4 (from whylogs==1.3.0->whylogs[viz]==1.3.0)\n",
      "  Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3.0,>=2.27 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs==1.3.0->whylogs[viz]==1.3.0) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.30.0.0 (from whylogs==1.3.0->whylogs[viz]==1.3.0)\n",
      "  Downloading types_requests-2.31.0.20240403-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs==1.3.0->whylogs[viz]==1.3.0) (4.9.0)\n",
      "Collecting whylabs-client<0.6.0,>=0.5.3 (from whylogs==1.3.0->whylogs[viz]==1.3.0)\n",
      "  Using cached whylabs_client-0.5.10-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting whylogs-sketching>=3.4.1.dev3 (from whylogs==1.3.0->whylogs[viz]==1.3.0)\n",
      "  Downloading whylogs_sketching-3.4.1.dev3-cp310-cp310-win_amd64.whl.metadata (441 bytes)\n",
      "Collecting Pillow<10.0.0,>=9.2.0 (from whylogs[viz]==1.3.0)\n",
      "  Downloading Pillow-9.5.0-cp310-cp310-win_amd64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: ipython in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from whylogs[viz]==1.3.0) (8.22.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs[viz]==1.3.0) (1.26.4)\n",
      "Collecting pybars3<0.10,>=0.9 (from whylogs[viz]==1.3.0)\n",
      "  Downloading pybars3-0.9.7.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy>=1.5 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs[viz]==1.3.0) (1.13.0)\n",
      "Collecting PyMeta3>=0.5.1 (from pybars3<0.10,>=0.9->whylogs[viz]==1.3.0)\n",
      "  Downloading PyMeta3-0.5.1.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests<3.0,>=2.27->whylogs==1.3.0->whylogs[viz]==1.3.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests<3.0,>=2.27->whylogs==1.3.0->whylogs[viz]==1.3.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests<3.0,>=2.27->whylogs==1.3.0->whylogs[viz]==1.3.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests<3.0,>=2.27->whylogs==1.3.0->whylogs[viz]==1.3.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from whylabs-client<0.6.0,>=0.5.3->whylogs==1.3.0->whylogs[viz]==1.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: decorator in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (5.14.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython->whylogs[viz]==1.3.0) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython->whylogs[viz]==1.3.0) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->whylogs[viz]==1.3.0) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil->whylabs-client<0.6.0,>=0.5.3->whylogs==1.3.0->whylogs[viz]==1.3.0) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->whylogs[viz]==1.3.0) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->whylogs[viz]==1.3.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython->whylogs[viz]==1.3.0) (0.2.2)\n",
      "Using cached whylogs-1.3.0-py3-none-any.whl (1.9 MB)\n",
      "Downloading Pillow-9.5.0-cp310-cp310-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB 660.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.5 MB 1.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.5 MB 1.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.5 MB 1.6 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.4/2.5 MB 1.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.5 MB 1.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.6/2.5 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.8/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.2/2.5 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.5/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.9/2.5 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "   ---------------------------------------- 0.0/420.9 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 307.2/420.9 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 420.9/420.9 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading types_requests-2.31.0.20240403-py3-none-any.whl (15 kB)\n",
      "Using cached whylabs_client-0.5.10-py3-none-any.whl (440 kB)\n",
      "Downloading whylogs_sketching-3.4.1.dev3-cp310-cp310-win_amd64.whl (382 kB)\n",
      "   ---------------------------------------- 0.0/382.9 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 256.0/382.9 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 382.9/382.9 kB 5.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pybars3, PyMeta3\n",
      "  Building wheel for pybars3 (setup.py): started\n",
      "  Building wheel for pybars3 (setup.py): finished with status 'done'\n",
      "  Created wheel for pybars3: filename=pybars3-0.9.7-py3-none-any.whl size=14093 sha256=d4ad3c109cfb64543da82d3eadbbb13a82920412e20ef5dba534b39898ade3cf\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\2f\\5b\\65\\505e94231d7dc278c9e0ddf8dbb1974cfb303eba742dbf55dd\n",
      "  Building wheel for PyMeta3 (setup.py): started\n",
      "  Building wheel for PyMeta3 (setup.py): finished with status 'done'\n",
      "  Created wheel for PyMeta3: filename=PyMeta3-0.5.1-py3-none-any.whl size=16488 sha256=eced0687f1c146ceb20c888b44f9fe608b0cc7851e6161228d71c26baf6ba1d0\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\f4\\ef\\62\\1d388a1576d871760164a1388632b29645c3f907cd009d6cb1\n",
      "Successfully built pybars3 PyMeta3\n",
      "Installing collected packages: whylogs-sketching, PyMeta3, types-requests, pybars3, protobuf, Pillow, whylabs-client, whylogs\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "Successfully installed Pillow-9.5.0 PyMeta3-0.5.1 protobuf-5.26.1 pybars3-0.9.7 types-requests-2.31.0.20240403 whylabs-client-0.5.10 whylogs-1.3.0 whylogs-sketching-3.4.1.dev3\n"
     ]
    }
   ],
   "source": [
    "!pip install whylogs[viz]==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca0e3d4-a0a0-4d43-bad0-42cbbb436384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langkit[all]\n",
      "  Downloading langkit-0.0.31-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting pandas (from langkit[all])\n",
      "  Downloading pandas-2.2.1-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting textstat<0.8.0,>=0.7.3 (from langkit[all])\n",
      "  Downloading textstat-0.7.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting whylogs<2.0.0,>=1.3.19 (from langkit[all])\n",
      "  Using cached whylogs-1.3.28-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting datasets<3.0.0,>=2.12.0 (from langkit[all])\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting detoxify<0.6.0,>=0.5.2 (from langkit[all])\n",
      "  Downloading detoxify-0.5.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting evaluate<0.5.0,>=0.4.0 (from langkit[all])\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting h5py<4.0.0,>=3.10.0 (from langkit[all])\n",
      "  Downloading h5py-3.10.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: ipywidgets<9.0.0,>=8.1.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from langkit[all]) (8.1.2)\n",
      "Collecting nltk<4.0.0,>=3.8.1 (from langkit[all])\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from langkit[all]) (1.26.4)\n",
      "Collecting openai<2.0.0,>=0.27.6 (from langkit[all])\n",
      "  Downloading openai-1.16.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting presidio-analyzer<3.0.0,>=2.2.351 (from langkit[all])\n",
      "  Downloading presidio_analyzer-2.2.354-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from langkit[all]) (2.6.1)\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from langkit[all]) (2.2.2)\n",
      "Collecting vadersentiment<4.0.0,>=3.3.2 (from langkit[all])\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (3.13.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets<3.0.0,>=2.12.0->langkit[all])\n",
      "  Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets<3.0.0,>=2.12.0->langkit[all])\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<3.0.0,>=2.12.0->langkit[all])\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (4.65.0)\n",
      "Collecting xxhash (from datasets<3.0.0,>=2.12.0->langkit[all])\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets<3.0.0,>=2.12.0->langkit[all])\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets<3.0.0,>=2.12.0->langkit[all]) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets<3.0.0,>=2.12.0->langkit[all]) (6.0.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from detoxify<0.6.0,>=0.5.2->langkit[all]) (4.37.2)\n",
      "Collecting sentencepiece>=0.1.94 (from detoxify<0.6.0,>=0.5.2->langkit[all])\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting responses<0.19 (from evaluate<0.5.0,>=0.4.0->langkit[all])\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets<9.0.0,>=8.1.1->langkit[all]) (8.22.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipywidgets<9.0.0,>=8.1.1->langkit[all]) (5.14.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.1->langkit[all]) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from ipywidgets<9.0.0,>=8.1.1->langkit[all]) (3.0.10)\n",
      "Collecting click (from nltk<4.0.0,>=3.8.1->langkit[all])\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->langkit[all]) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->langkit[all]) (2023.10.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from openai<2.0.0,>=0.27.6->langkit[all]) (4.2.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=0.27.6->langkit[all])\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=0.27.6->langkit[all])\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from openai<2.0.0,>=0.27.6->langkit[all]) (2.6.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from openai<2.0.0,>=0.27.6->langkit[all]) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from openai<2.0.0,>=0.27.6->langkit[all]) (4.9.0)\n",
      "Collecting spacy<4.0.0,>=3.4.4 (from presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting tldextract (from presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting phonenumbers<9.0.0,>=8.12 (from presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading phonenumbers-8.13.33-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from sentence-transformers<3.0.0,>=2.2.2->langkit[all]) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from sentence-transformers<3.0.0,>=2.2.2->langkit[all]) (1.13.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from sentence-transformers<3.0.0,>=2.2.2->langkit[all]) (9.5.0)\n",
      "Collecting pyphen (from textstat<0.8.0,>=0.7.3->langkit[all])\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch->langkit[all]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch->langkit[all]) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch->langkit[all]) (3.1.3)\n",
      "Requirement already satisfied: platformdirs<4.0.0,>=3.5.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs<2.0.0,>=1.3.19->langkit[all]) (3.10.0)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs<2.0.0,>=1.3.19->langkit[all]) (5.26.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.30.0.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs<2.0.0,>=1.3.19->langkit[all]) (2.31.0.20240403)\n",
      "Collecting whylabs-client<0.7.0,>=0.6.0 (from whylogs<2.0.0,>=1.3.19->langkit[all])\n",
      "  Using cached whylabs_client-0.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: whylogs-sketching>=3.4.1.dev3 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from whylogs<2.0.0,>=1.3.19->langkit[all]) (3.4.1.dev3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas->langkit[all]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->langkit[all]) (2023.3.post1)\n",
      "Collecting tzdata>=2022.7 (from pandas->langkit[all])\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.27.6->langkit[all]) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=0.27.6->langkit[all]) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets<3.0.0,>=2.12.0->langkit[all]) (4.0.3)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=0.27.6->langkit[all]) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=0.27.6->langkit[all])\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=0.27.6->langkit[all])\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (2.17.2)\n",
      "Requirement already satisfied: stack-data in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=0.27.6->langkit[all]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from pydantic<3,>=1.9.0->openai<2.0.0,>=0.27.6->langkit[all]) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->langkit[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.12.0->langkit[all]) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests>=2.19.0->datasets<3.0.0,>=2.12.0->langkit[all]) (2.1.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all]) (68.2.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers->detoxify<0.6.0,>=0.5.2->langkit[all]) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers->detoxify<0.6.0,>=0.5.2->langkit[all]) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from jinja2->torch->langkit[all]) (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->langkit[all]) (3.4.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from sympy->torch->langkit[all]) (1.3.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.2.13)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.4.4->presidio-analyzer<3.0.0,>=2.2.351->langkit[all])\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9.0.0,>=8.1.1->langkit[all]) (0.2.2)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 194.6/510.5 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 510.5/510.5 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading detoxify-0.5.2-py3-none-any.whl (12 kB)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB ? eta 0:00:00\n",
      "Downloading h5py-3.10.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.7 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.7/2.7 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.7 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.7 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.7 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.4/2.7 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.7 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.8/2.7 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.0/2.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.1/2.7 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.1/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.1/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.3/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.5/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.7 MB/s eta 0:00:00\n",
      "Downloading openai-1.16.1-py3-none-any.whl (266 kB)\n",
      "   ---------------------------------------- 0.0/266.9 kB ? eta -:--:--\n",
      "   ---------------------------------------  266.2/266.9 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 266.9/266.9 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading presidio_analyzer-2.2.354-py3-none-any.whl (92 kB)\n",
      "   ---------------------------------------- 0.0/92.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 92.2/92.2 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.1/105.1 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 126.0/126.0 kB ? eta 0:00:00\n",
      "Using cached whylogs-1.3.28-py3-none-any.whl (1.9 MB)\n",
      "Downloading langkit-0.0.31-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.6/1.2 MB 12.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.8/1.2 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.1/1.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.1-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.6 MB 7.0 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/11.6 MB 5.4 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.7/11.6 MB 4.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/11.6 MB 4.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.1/11.6 MB 4.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/11.6 MB 4.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.6 MB 4.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 4.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.8/11.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/11.6 MB 4.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.3/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.7/11.6 MB 4.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.0/11.6 MB 4.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.2/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.6/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.9/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.1/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 4.3/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.4/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.6/11.6 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.8/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.9/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.1/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.3/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.7/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.0/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.2/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.3/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.5/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.7/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.0/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.2/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.4/11.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.5/11.6 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.7/11.6 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.9/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.2/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.6/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.7/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.9/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.1/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.2/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.7/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.9/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.1/11.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.3/11.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.5/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.6/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.8/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.1/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.2/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.4/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 0.0/75.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.6/75.6 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.9/77.9 kB ? eta 0:00:00\n",
      "Downloading phonenumbers-8.13.33-py2.py3-none-any.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.6 MB 9.8 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.6/2.6 MB 6.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.6 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.0/2.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.2/2.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.6 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.6/2.6 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.7/2.6 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.9/2.6 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 2.0/2.6 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.1/2.6 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.3/2.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.4/2.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.6/2.6 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.2-cp310-cp310-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/24.8 MB 7.9 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.4/24.8 MB 5.4 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.6/24.8 MB 4.8 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.7/24.8 MB 4.2 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.9/24.8 MB 4.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.1/24.8 MB 4.0 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.2/24.8 MB 3.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.4/24.8 MB 3.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.5/24.8 MB 3.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.7/24.8 MB 3.6 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.8/24.8 MB 3.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.0/24.8 MB 3.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.1/24.8 MB 3.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.3/24.8 MB 3.6 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.4/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.6/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.8/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.1/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.2/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.4/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.6/24.8 MB 3.5 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.7/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 3.9/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.1/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 4.3/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.5/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.6/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 4.8/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.0/24.8 MB 3.6 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.1/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.3/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.4/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.6/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.8/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 5.9/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.1/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.2/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.4/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.5/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.7/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 6.9/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.0/24.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 7.4/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.6/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.7/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 7.9/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.1/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.3/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.4/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 8.6/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 8.8/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.0/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.1/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.3/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.5/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.6/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.8/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 9.9/24.8 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.1/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.2/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.4/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.6/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.7/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 10.9/24.8 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.0/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.2/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.3/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.5/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 11.6/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 11.8/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.0/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.1/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 12.3/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.4/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.6/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.7/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 12.9/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 13.0/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.2/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.3/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 13.5/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.7/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 13.8/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 14.0/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 14.1/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.3/24.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.4/24.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.6/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 14.7/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 14.9/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.1/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.2/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 15.4/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.5/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.7/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 15.8/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 16.0/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.1/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.3/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.5/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.6/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.8/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 16.9/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.1/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.2/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.4/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.5/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.7/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.0/24.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.2/24.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.3/24.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.5/24.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.6/24.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.8/24.8 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.9/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.1/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.3/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.4/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.6/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 19.9/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.0/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.2/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.3/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.5/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.8/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.2/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.4/24.8 MB 3.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.5/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.7/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.9/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.0/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.2/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.4/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.7/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.9/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.7/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 440.3/991.5 kB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 614.4/991.5 kB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 788.5/991.5 kB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 942.1/991.5 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB 5.7 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.4/12.1 MB 4.6 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.8/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.1/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 3.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.0/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.1/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.3/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.5/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.6/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.8/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.6/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.0/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.3/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.5/12.1 MB 3.7 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.7/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.6/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.3/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.2/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 7.4/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.6/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.8/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.9/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 8.1/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.3/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.0/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.7/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.0/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.9/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 3.8 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   --------------------------------------  337.9/345.4 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 345.4/345.4 kB 7.1 MB/s eta 0:00:00\n",
      "Using cached whylabs_client-0.6.1-py3-none-any.whl (436 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.6/2.0 MB 17.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.0 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.0 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.1/2.0 MB 6.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/2.0 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.7/2.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.9/2.0 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.6/97.6 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.6/181.6 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 481.9/481.9 kB 15.2 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 6.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 0.9/1.5 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.3/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.6/6.6 MB 12.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.8/6.6 MB 8.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.0/6.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/6.6 MB 5.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.3/6.6 MB 5.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.5/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 1.8/6.6 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.0/6.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 2.4/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.6/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.5/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.6/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.8/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.0/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.2/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.5/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 4.7/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 5.1/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.3/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.4/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.6/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.8/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.0/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.2/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.5/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.3/58.3 kB ? eta 0:00:00\n",
      "Installing collected packages: sentencepiece, phonenumbers, cymem, xxhash, wasabi, tzdata, spacy-loggers, spacy-legacy, smart-open, pyphen, pyarrow-hotfix, pyarrow, murmurhash, langcodes, h5py, h11, distro, dill, cloudpathlib, click, catalogue, blis, whylabs-client, vadersentiment, typer, textstat, srsly, responses, requests-file, preshed, pandas, nltk, multiprocess, httpcore, whylogs, tldextract, httpx, confection, weasel, thinc, openai, langkit, datasets, spacy, evaluate, detoxify, presidio-analyzer\n",
      "  Attempting uninstall: whylabs-client\n",
      "    Found existing installation: whylabs-client 0.5.10\n",
      "    Uninstalling whylabs-client-0.5.10:\n",
      "      Successfully uninstalled whylabs-client-0.5.10\n",
      "  Attempting uninstall: whylogs\n",
      "    Found existing installation: whylogs 1.3.0\n",
      "    Uninstalling whylogs-1.3.0:\n",
      "      Successfully uninstalled whylogs-1.3.0\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 datasets-2.18.0 detoxify-0.5.2 dill-0.3.8 distro-1.9.0 evaluate-0.4.1 h11-0.14.0 h5py-3.10.0 httpcore-1.0.5 httpx-0.27.0 langcodes-3.3.0 langkit-0.0.31 multiprocess-0.70.16 murmurhash-1.0.10 nltk-3.8.1 openai-1.16.1 pandas-2.2.1 phonenumbers-8.13.33 preshed-3.0.9 presidio-analyzer-2.2.354 pyarrow-15.0.2 pyarrow-hotfix-0.6 pyphen-0.14.0 requests-file-2.0.0 responses-0.18.0 sentencepiece-0.2.0 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 textstat-0.7.3 thinc-8.2.3 tldextract-5.1.2 typer-0.9.4 tzdata-2024.1 vadersentiment-3.3.2 wasabi-1.1.2 weasel-0.3.4 whylabs-client-0.6.1 whylogs-1.3.28 xxhash-3.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "832cc2cc-4961-49ca-beca-ca7ba209d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.8.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from trl) (2.2.2)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from trl) (4.37.2)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from trl) (1.26.4)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from trl) (0.29.0.dev0)\n",
      "Requirement already satisfied: datasets in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from trl) (2.18.0)\n",
      "Collecting tyro>=0.5.11 (from trl)\n",
      "  Using cached tyro-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from torch>=1.4.0->trl) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from transformers>=4.31.0->trl) (4.65.0)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
      "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
      "  Using cached shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from tyro>=0.5.11->trl) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from accelerate->trl) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (2.2.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from datasets->trl) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from aiohttp->datasets->trl) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from requests->transformers>=4.31.0->trl) (2024.2.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets->trl) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Using cached trl-0.8.1-py3-none-any.whl (225 kB)\n",
      "Using cached tyro-0.7.3-py3-none-any.whl (79 kB)\n",
      "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   ---- ---------------------------------- 30.7/240.7 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 225.3/240.7 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 240.7/240.7 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB 4.8 MB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: shtab, mdurl, docstring-parser, markdown-it-py, rich, tyro, trl\n",
      "Successfully installed docstring-parser-0.16 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.1 shtab-1.7.1 trl-0.8.1 tyro-0.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install trl\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eca92bb-88dc-4757-a1cb-bc9488c42e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (1.4.0)\n",
      "Collecting numba\n",
      "  Downloading numba-0.59.1-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.42.0-cp310-cp310-win_amd64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in c:\\users\\hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages (from numba) (1.26.4)\n",
      "Downloading numba-0.59.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.7 MB 487.6 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.2/2.7 MB 1.6 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.5/2.7 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.9/2.7 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.1/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 4.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.3/2.7 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.7 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.4/2.7 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.3/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.5/2.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 3.7 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.42.0-cp310-cp310-win_amd64.whl (28.1 MB)\n",
      "   ---------------------------------------- 0.0/28.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/28.1 MB 8.5 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.6/28.1 MB 6.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.8/28.1 MB 5.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.8/28.1 MB 5.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.8/28.1 MB 5.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.8/28.1 MB 5.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.0/28.1 MB 3.1 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.5/28.1 MB 4.3 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/28.1 MB 3.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.6/28.1 MB 3.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.7/28.1 MB 3.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.4/28.1 MB 4.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.6/28.1 MB 4.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.7/28.1 MB 4.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/28.1 MB 4.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.1/28.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.3/28.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.5/28.1 MB 4.1 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.7/28.1 MB 4.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.8/28.1 MB 4.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.9/28.1 MB 4.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 4.1/28.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 4.2/28.1 MB 4.0 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/28.1 MB 3.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.5/28.1 MB 3.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.7/28.1 MB 3.8 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.8/28.1 MB 3.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.0/28.1 MB 3.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.1/28.1 MB 3.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.3/28.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.4/28.1 MB 3.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.6/28.1 MB 3.7 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.8/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 5.9/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 6.1/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 6.2/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.4/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.5/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.7/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 6.9/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 7.0/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.2/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.3/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.5/28.1 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.6/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.8/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 8.0/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 8.2/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 8.3/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.4/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.6/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.8/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.9/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 9.1/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.2/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.4/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.6/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 9.7/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 9.9/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.0/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.2/28.1 MB 3.6 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.4/28.1 MB 3.5 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 10.5/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.7/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.9/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 11.1/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 11.2/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.4/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.6/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.8/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.9/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.1/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.3/28.1 MB 3.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.5/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 12.6/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.8/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 13.0/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 13.1/28.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 13.3/28.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.5/28.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.6/28.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.8/28.1 MB 3.4 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 14.0/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 14.2/28.1 MB 3.5 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 14.3/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 14.5/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 14.7/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 14.9/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 15.0/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 15.2/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 15.4/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 15.6/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 15.7/28.1 MB 3.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 15.9/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 16.1/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.2/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.4/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.6/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 16.8/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.9/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 17.1/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 17.3/28.1 MB 3.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 17.4/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 17.6/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 17.7/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 17.9/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 18.1/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 18.3/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.4/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.6/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.8/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 18.9/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.1/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.3/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.4/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 19.6/28.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 19.8/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 20.0/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 20.1/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 20.3/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 20.5/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 20.7/28.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 20.8/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 21.0/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.2/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.4/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.5/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 21.7/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 21.9/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.0/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.2/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 22.4/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.5/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.7/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 22.9/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 23.1/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.2/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.4/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.6/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 23.8/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 23.9/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.1/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.3/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.4/28.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 24.6/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 24.8/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.0/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.1/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 25.3/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.5/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.6/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 25.8/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 26.0/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.2/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.3/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.5/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 26.7/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 26.9/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.0/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.2/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 27.4/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.5/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.7/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  27.9/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.1/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  28.1/28.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.1/28.1 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: llvmlite, numba\n",
      "Successfully installed llvmlite-0.42.0 numba-0.59.1\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a337f3-5d80-4535-b99a-ddfae9afcb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import gc\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "    print(\"Clear garbage collection:-- \",gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6526a8c0-72ca-4140-88f4-2b8509da99c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  0% |  0% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  4% |  2% |\n",
      "Clear garbage collection:--  23\n"
     ]
    }
   ],
   "source": [
    "free_gpu_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1a46167-a0c2-4f2a-b9fd-e7465e5ea832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d85e76dcfd415c8c69da25625c4e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hp\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540d23829d744c378b9ebdbe51656909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74e5ba6d24349a8b2ad734e936c43cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db47c2964f034b92a186a8cb97c97491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f75ac3879941219652a734804069c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b20ef3aeb54a4aaf62c4074aa3dc46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c2557d60e94e8e937a432172a6901d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8e1df0c07e47869dd1ed866cc4f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa024e36853b41b780111e863d4f6f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732681094d5a49238885b09a695b6738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538684682ae345318b347ec54e286bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a57655084bc4880a5fdf01c6a346577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/403 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hp\\.cache\\huggingface\\hub\\models--martin-ha--toxic-comment-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a94d0ad4654feb8d0f5d5c32d08f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cf6b0109ce439489f9f0e9c2f778da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eadb467339c4deb927d0151be3269c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470607a3505745de8e2063a6c4d5b921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffbb4f7f998423286cb618cf0e1e02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,LlamaTokenizer\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from whylogs.api.writer.whylabs import WhyLabsWriter\n",
    "from langkit import llm_metrics\n",
    "import whylogs as why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3178ea-c686-465e-a03f-932093692bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fed741-b7b9-46ca-bc9e-c514a4ce58ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1661a2-df43-449d-b573-f6991c3a5ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8104d10-836f-4ae5-986e-1f4e4f570c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = 'org-X9EnPK'\n",
    "os.environ[\"WHYLABS_API_KEY\"] = 'your_api_key'\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = 'model-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "141b5da7-69a1-4fe6-b399-4974c41d8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"C:/Users/Hp/llama/llama-2-7b-chat-hf\"\n",
    "adapters_name = 'Trelis/Llama-2-7b-chat-hf-function-calling-adapters-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b2e708-8574-482c-8ecd-62461fc2a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_data = \"C:/Users/Hp/llama2_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bbd439c-a08c-4bef-a4fc-28508cf599ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The instruction dataset to use\n",
    "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "\n",
    "# # Load dataset (you can process it here)\n",
    "# dataset = load_dataset(dataset_name, split=\"train\")\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f30296-1558-4472-b1b4-65ebfc350e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Provide the local file path to your dataset\n",
    "dataset_path = dataset_data\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"csv\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# Now you can access your dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8006f372-7b04-4e5e-a3fd-9872ce3929c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New line \n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-2-7b-chat-finetune\"\n",
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6973ca0-f797-4cdc-9811-fd8f11ec02a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65e52b3c6794f50a8149f1805f71928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120e2bd2-2529-4ae8-aa30-27dd2f6a22c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:670: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=7.227375030517578, metrics={'train_runtime': 28.2697, 'train_samples_per_second': 0.071, 'train_steps_per_second': 0.035, 'total_flos': 1832887861248.0, 'train_loss': 7.227375030517578, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#New Line\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38395eb7-d5c3-472a-b384-48848b09f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a45a59-e7a4-493f-8f90-7c5ef7e2234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f8f6a25-853b-4995-a4a8-5b38920fb0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the model C:/Users/Hp/llama/llama-2-7b-chat-hf into memory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f19db645d4c4cbb245271c2f22445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca039450b34f958063b1cbf9a3f578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Hp\\.cache\\huggingface\\hub\\models--Trelis--Llama-2-7b-chat-hf-function-calling-adapters-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55af56a67104e3aad80c5384a0beb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the model C:/Users/Hp/llama/llama-2-7b-chat-hf into memory\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig\n",
    "from torch import cuda, bfloat16\n",
    "#model_dir = \"C:/Users/Hp/llama/llama-2-7b-chat-hf\"\n",
    "\n",
    "model_dir = \"C:/Users/Hp/llama/llama-2-7b-chat-hf\"\n",
    "#adapters_name = 'RuterNorway/Llama-2-7b-chat-norwegian-LoRa'\n",
    "adapters_name = 'Trelis/Llama-2-7b-chat-hf-function-calling-adapters-v2'\n",
    "\n",
    "print(f\"Starting to load the model {model_dir} into memory\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_dir,padding = True,truncation=True,return_tensors='pt')\n",
    "model =AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "    ),    \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0} \n",
    ")\n",
    "model=PeftModel.from_pretrained(model, adapters_name)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "tokenizer.bos_token_id = 1\n",
    "stop_token_ids = [0]\n",
    "\n",
    "print(f\"Successfully loaded the model {model_dir} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8645acfb-c2bc-43df-87d2-f492931de229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure special tokens are set\n",
    "tokenizer.bos_token_id = 1\n",
    "tokenizer.eos_token_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b30c8bd-7d43-444a-b271-7d477d3d96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for text generation\n",
    "llama_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9656e90-df28-48d3-b56a-b47c4758ebdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96206017-4bf6-4897-8365-482bbc0a7ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ddbaa-22cf-4b9e-ae0c-45d4794aabb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8bfd8-67d0-4732-8bc6-e8fd5aab8bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da5a27-3d17-4417-938a-d1e337bebc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447e2b7-8d3a-498b-83db-715aa089aca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930d06c5-220b-4f7d-89cd-96c5a5fa057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful bot. Your answer are clear and concise.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "#Formatting function for message and history\n",
    "def format_message(message: str, history:list, memory_limit:int=3) -> str:\n",
    "  \"\"\"\n",
    "  Formats are the message and history for the llama model\n",
    "\n",
    "  Parameters:\n",
    "        message(str): Current message to send\n",
    "        history(list): Past conversation history\n",
    "        memory_limit(int): Limit on how many past interactions to consider.\n",
    "\n",
    "  Returns:\n",
    "        str: Formatted message string\n",
    "  \"\"\"\n",
    "\n",
    "  #always keep len(history) <=memory_limit\n",
    "  if len(history)>memory_limit:\n",
    "    history=history[-memory_limit:]\n",
    "\n",
    "  if len(history)==0:\n",
    "    return SYSTEM_PROMPT + f\"{message} [/INST]\"\n",
    "\n",
    "  formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
    "\n",
    "  #Handle conversation history\n",
    "  for user_msg, model_answer in history[1:]:\n",
    "    formatted_message += f\"<s>[INST]{user_msg}[/INST] {model_answer} </s>\"\n",
    "\n",
    "  #Handle the current message\n",
    "  formatted_message += f\"<s>[INST] {message} [/INST]\"\n",
    "\n",
    "  return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e4798-c37b-4c85-b34d-8ba6b7cbbfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8448586-5cdc-4052-a8fe-79908ebba380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "prompt = {}\n",
    "schema = llm_metrics.init()\n",
    "async def setup_telemetry(message,response):\n",
    "    telemetry_agent = WhyLabsWriter()\n",
    "    prompt = {\n",
    "        \"prompt\":message,\n",
    "        \"response\":response.strip()\n",
    "    }\n",
    "\n",
    "    profile=why.log(prompt,schema=schema).profile()\n",
    "    profile.set_dataset_timestamp(datetime.datetime.now())\n",
    "\n",
    "    telemetry_agent.write(profile.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e79969-d222-40ea-aace-2ec4d5b56e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(message:str, history: list) -> str:\n",
    "  \"\"\"\n",
    "  Generates a conversational response from the llama model.\n",
    "\n",
    "  Parameters:\n",
    "        message(str): user input message\n",
    "        history(list): Past conversation history\n",
    "\n",
    "  Returns:\n",
    "        str: generated response from the llama model\n",
    "  \"\"\"\n",
    "  query = format_message_new(message,history)\n",
    "  response = \"\"\n",
    "\n",
    "  sequences = llama_pipeline(\n",
    "      query,\n",
    "      do_sample=True,\n",
    "      top_k = 10,\n",
    "      num_return_sequences=1,\n",
    "      max_length=1024,\n",
    "      truncation=True,\n",
    "  )\n",
    "  generated_text = sequences[0]['generated_text']\n",
    "  response = generated_text[len(query):] #Removing prompt from the output\n",
    "  print(\"Chatbot: \",response.strip())\n",
    "  print(response)\n",
    "  print(datetime.datetime.now())\n",
    "\n",
    "  setup_telemetry(message,response)\n",
    "\n",
    "  return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc70f8e9-0b5f-46fa-a5af-9142a8876ce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mget_llama_response\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYour name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mget_llama_response\u001b[1;34m(message, history)\u001b[0m\n\u001b[0;32m     12\u001b[0m query \u001b[38;5;241m=\u001b[39m format_message_new(message,history)\n\u001b[0;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m sequences \u001b[38;5;241m=\u001b[39m \u001b[43mllama_pipeline\u001b[49m(\n\u001b[0;32m     16\u001b[0m     query,\n\u001b[0;32m     17\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     18\u001b[0m     top_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     19\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     20\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m     21\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m sequences[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m generated_text[\u001b[38;5;28mlen\u001b[39m(query):] \u001b[38;5;66;03m#Removing prompt from the output\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'llama_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "get_llama_response(\"Your name\",[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c39de014-f8b6-4fa8-8ab1-45c8ca6e1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5eb9fe7-a64d-458b-88cc-7ac1c5e87042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Hello! Nice to meet you. Is there anything I can help you with?\n",
      "  Hello! Nice to meet you. Is there anything I can help you with?\n",
      "2024-04-03 15:43:33.143617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_4728\\5332401.py:29: RuntimeWarning: coroutine 'setup_telemetry' was never awaited\n",
      "  setup_telemetry(message,response)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Great! Could you please elaborate or provide more context on what you need help with? I'm here to assist you with any questions or tasks you may have.\n",
      " Great! Could you please elaborate or provide more context on what you need help with? I'm here to assist you with any questions or tasks you may have.\n",
      "2024-04-03 15:44:00.501792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Sure! To swap two numbers in Java, you can use the following code:\n",
      "```\n",
      "public static void swap(int a, int b) {\n",
      "    int temp = a;\n",
      "    a = b;\n",
      "    b = temp;\n",
      "}\n",
      "```\n",
      "You can call this method by passing in the two numbers you want to swap, like this:\n",
      "```\n",
      "swap(a, b);\n",
      "```\n",
      "Alternatively, you can use the `Math.min()` method to swap two numbers, like this:\n",
      "```\n",
      "int temp = Math.min(a, b);\n",
      "a = Math.max(a, temp);\n",
      "b = Math.max(b, temp);\n",
      "```\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      " Sure! To swap two numbers in Java, you can use the following code:\n",
      "```\n",
      "public static void swap(int a, int b) {\n",
      "    int temp = a;\n",
      "    a = b;\n",
      "    b = temp;\n",
      "}\n",
      "```\n",
      "You can call this method by passing in the two numbers you want to swap, like this:\n",
      "```\n",
      "swap(a, b);\n",
      "```\n",
      "Alternatively, you can use the `Math.min()` method to swap two numbers, like this:\n",
      "```\n",
      "int temp = Math.min(a, b);\n",
      "a = Math.max(a, temp);\n",
      "b = Math.max(b, temp);\n",
      "```\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "2024-04-03 15:45:55.572064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Great! Is there anything else you would like to know about swapping numbers in Java?\n",
      " Great! Is there anything else you would like to know about swapping numbers in Java?\n",
      "2024-04-03 15:47:06.591186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  The full form of TCS is Tata Consultancy Services.\n",
      " The full form of TCS is Tata Consultancy Services.\n",
      "2024-04-03 15:49:01.910990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  TCS NQT stands for Tata Consultancy Services National Qualifier Test. It is a qualifying test conducted by TCS for recruiting freshers into their company. The test aims to identify and select the best talent from various engineering and technology streams in India. TCS NQT is a multiple-choice question (MCQ) based test that covers topics such as maths, logic, and verbal ability.\n",
      " TCS NQT stands for Tata Consultancy Services National Qualifier Test. It is a qualifying test conducted by TCS for recruiting freshers into their company. The test aims to identify and select the best talent from various engineering and technology streams in India. TCS NQT is a multiple-choice question (MCQ) based test that covers topics such as maths, logic, and verbal ability.\n",
      "2024-04-03 15:50:03.255314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Good! Is there anything else you would like to know about TCS NQT?\n",
      " Good! Is there anything else you would like to know about TCS NQT?\n",
      "2024-04-03 15:50:27.947528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  Sure! The TCS NQT exam pattern consists of multiple-choice questions (MCQs) divided into three sections:\n",
      "1. Quantitative Aptitude (QA) - 50 questions, 50 marks\n",
      "Topics covered: Numbers and Algebra, Geometry and Trigonometry, Statistics and Miscellaneous\n",
      "2. Logical Reasoning and Analytical Thinking (LRAT) - 30 questions, 30 marks\n",
      "Topics covered: Pattern recognition, Logical Reasoning, and Analytical Thinking\n",
      "3. Language Proficiency (LP) - 20 questions, 20 marks\n",
      "Topics covered: English Language and Basic Grammar\n",
      "The total exam duration is 1 hour and 30 minutes. The exam is conducted online and you can choose your preferred language for the LP section.\n",
      "You can practice TCS NQT sample questions and mock tests to get acquainted with the exam pattern and improve your chances of clearing the exam.\n",
      " Sure! The TCS NQT exam pattern consists of multiple-choice questions (MCQs) divided into three sections:\n",
      "1. Quantitative Aptitude (QA) - 50 questions, 50 marks\n",
      "Topics covered: Numbers and Algebra, Geometry and Trigonometry, Statistics and Miscellaneous\n",
      "2. Logical Reasoning and Analytical Thinking (LRAT) - 30 questions, 30 marks\n",
      "Topics covered: Pattern recognition, Logical Reasoning, and Analytical Thinking\n",
      "3. Language Proficiency (LP) - 20 questions, 20 marks\n",
      "Topics covered: English Language and Basic Grammar\n",
      "The total exam duration is 1 hour and 30 minutes. The exam is conducted online and you can choose your preferred language for the LP section.\n",
      "You can practice TCS NQT sample questions and mock tests to get acquainted with the exam pattern and improve your chances of clearing the exam.\n",
      "2024-04-03 15:52:40.138351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  You're welcome! Is there anything else you would like to know about TCS NQT or placement tests in general? Let me know!\n",
      " You're welcome! Is there anything else you would like to know about TCS NQT or placement tests in general? Let me know!\n",
      "2024-04-03 15:53:33.739571\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(get_llama_response).launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f68767-7f00-4bd9-ae4e-6fd5c9aa6c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
