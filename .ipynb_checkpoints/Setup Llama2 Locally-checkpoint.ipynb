{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e598d-05d2-4db2-ac82-89be76222d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c7ca29-a37c-4029-9b6d-add894851976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969baf7-6cf6-47f1-b20e-516dc87dabd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0f0971-48d2-4eea-9b37-3c212750de03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50fdf823-c365-4146-b749-64f74fe7e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run download.sh file via Git or WSL(Windows Subsystem for Linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391ea54-66a8-4efe-8afb-1502d1ba8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For below line run as administrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e4d39c-da6d-4348-824a-c3ac5d67a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "source = \"C:/Users/Hp/llama/tokenizer.model\"\n",
    "target = \"C:/Users/Hp/llama/llama-2-7b-chat/tokenizer.model\"\n",
    "\n",
    "# Create symbolic link\n",
    "os.symlink(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab0e844-6471-4c61-85bb-b10506cffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a323f1-0240-4792-8805-a1fc8b8ce285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command output: Fetching all parameters from the checkpoint at C:/Users/Hp/llama/llama-2-7b-chat.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Saving in the Transformers format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Assuming `TRANSFORM_SCRIPT_PATH` is the correct path to `convert_llama_weights_to_hf.py`\n",
    "TRANSFORM_SCRIPT_PATH = r\"C:\\Users\\Hp\\miniconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\models\\llama\\convert_llama_weights_to_hf.py\"\n",
    "\n",
    "command = [\n",
    "    \"python\", TRANSFORM_SCRIPT_PATH, \n",
    "    \"--input_dir\", \"C:/Users/Hp/llama/llama-2-7b-chat\", \n",
    "    \"--model_size\", \"7B\", \n",
    "    \"--output_dir\", \"C:/Users/Hp/llama/llama-2-7b-chat-hf\"\n",
    "]\n",
    "\n",
    "# Now run the command with subprocess.run\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"Error running command:\", result.stderr)\n",
    "else:\n",
    "    print(\"Command output:\", result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886b28d-4b5a-4802-a37b-cac977568ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
